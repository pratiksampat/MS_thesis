% \section{Introduction}
% \label{sec:introduction}

Precise allocation of CPUs is crucial in the cloud setting. Cloud providers can utilize this information to efficiently provision these resources and schedule applications accordingly. As a result, users experience predictable performance and costs while running their workloads. Poor estimation of CPU limits can lead to either a loss in performance due to throttling or CPU slack, where allocated but unused CPUs collectively lower the efficiency of the data center. Estimating CPU limits upfront, however, is non-trivial especially in dynamic behavior setting where load spikes may occur in an instant. Traditionally, the user would over allocate resources to prevent performance penalities. However, this led to significantly higher costs, as well as poor utilization of the data centers resources with analysis from Google's clusters \cite{reiss_heterogeneity_nodate} observing an average CPU utilization of 60\% and Alibaba\cite {lu_imbalance_2017} observing the average CPU utilization to not exceed over 40\%.

In today's cloud landscape, users widely adopt autoscaling techniques to optimize resource utilization and cost eff1iciency. Major cloud providers like Google \cite{rzadca_autopilot_2020}, Amazon \cite{noauthor_aws_nodate}, Microsoft \cite{edb-msft_autoscale_2023}, and IBM \cite{noauthor_ibm_nodate} offer autoscaling capabilities for resource management. Additionally, the Kubernetes Vertical Pod Autoscaler (K8s VPA) \cite{noauthor_kubernetes_nodate} has emerged as a popular open-source solution in the industry. K8s VPA continuously monitors resource utilization within defined windows, providing recommendations based on statistical analyses such as 50th and 95th percentiles of past CPU usage. Cloud providers may employ various techniques, including rules-based tuning and machine learning algorithms, to accurately estimate and suggest CPU allocations for optimal performance and cost savings.

% The rise of specialized autoscaling techniques designed to target specific application behaviors \pratik{CITE FIRM, COLA, SINAN}, suggests that the problem of efficiently scaling resources remains relevant.

We observe, that there exists a fundamental discrepancy between cloud interfaces and the OS Cgroup interface. The Linux Cgroup bandwidth controller necessitates CPU limits to be specified in terms of quota and period bandwidth. However, cloud users typically request CPU limits using millicpu or vCPU, representing a percentage ratio of quota to period. Presently, leading autoscaling solutions focus solely on adjusting the quota based on observed metrics, while the period often remains constant, typically defaulting to 100 ms. This approach leads to inaccurate resource estimations, resulting in suboptimal performance with high levels of throttling, potentially leading to over-entitlement recommended by the autoscalers in the future.

New autoscaling techniques have emerged, aiming to enhance traditional threshold and utilization-based methods. These approaches leverage queuing theory, reinforcement learning, and introduce novel metrics like performance awareness. However, despite these advancements, these techniques do not account for tuning the combination of both quota and period. Along with that, these techniques also encounter issues related to reactivity and precision when making recommendations further lowering their efficacy. Due to these inefficiencies, some cloud developers requested for interfaces \cite{noauthor_fix_nodate} from orchestrators like Kubernetes to manually tune for both quota and period, while some users also called for not using quota-period mechanism altogether \cite{noauthor_avoid_nodate} \cite{noauthor_requests_2023} \cite{noauthor_for_nodate} for when dealing with dynamic workload when their periodicity is not close to default.

The core problem is that userspace autoscalers rely on surrogate metrics as a proxy to model the application behavior. The application runtime behavior while opaque to the userspace is fairly transparent within the OS kernel. A task's behavior in terms of admittance, runtime and yield can be traced within the scheduler which can be further be used to accurately model the current application behavior. We propose \textit{CATCloud}, an in-kernel CPU auto tuning solution that profiles the OS scheduler to extract the application behavior and recommend quota and period based on past behavior. CATCloud presents a lightweight approach to tracing and recommendation by minimally instrumenting the Linux bandwidth scheduler to track runqueue statistics of runtime, yield and collates them at the Cgroup-level. CATCloud can operate in the granularity of milliseconds and employs simple statistical techniques to recommend future CPU limits.

Our proposed solution addresses two primary challenges. Firstly, accurately modeling runtime behavior proves to be a complex task. Instead of relying on surrogate metrics like CPU utilization or throttle count, CATCloud extracts application runtime behavior directly from the OS scheduler. While the scheduler manages operations such as running and yielding, extracting these metrics and modeling application behavior presents difficulties. Tracing bandwidth runtime poses challenges due to the scheduler's nature of task-switching based on metrics like context switches, IO wait times, and vruntime slice expiration. These fragmented runtime and yield slices fail to accurately represent actual application runtime behavior. Initially, we outline a straw-man approach involving period-bound tracing to gather this information and highlighting its limitations using microbenchmarks. Subsequently, we introduce a novel technique called period-agnostic tracing to track fragmented runtime and yield data. This data is then aggregated across multiple cores where the application concurrently runs. The application's runtime quota is modeled as the summation of estimated runtimes, while the period encompasses the summation of runtime and worst-case yield duration.

Secondly, contemporary CPU autoscaling solutions, apart from overlooking the period parameter, often rely on heavyweight algorithms like Machine Learning and Reinforcement Learning. These algorithms demand extensive telemetry data and time to build models, resulting in slow and sluggish reactivity (several minutes) when suggesting recommendations. This delay becomes particularly undesirable when dealing with workloads exhibiting high levels of dynamic utilization. CATCloud offers a contrasting approach with its lightweight tracing infrastructure, enabling millisecond-scale tracing. This results in significantly heightened reactivity when observing spikes in application load. Moreover, CATCloud empowers users to fine-tune the level of reactivity based on their domain-specific knowledge of application behavior.

We implement CATCloud as an extension to the Linux bandwidth scheduler, providing CATCloud interfaces through the Linux CPU Cgroup. These interfaces can be leveraged by container orchestrators to enable monitoring and recommendations. To assess CATCloud's effectiveness, we deploy a custom Linux kernel on QEMU x86 KVM and utilize Kubernetes as the cloud orchestrator. Our experiment encompasses a diverse range of cloud applications, including microservices, streaming, and serverless architectures, each exhibiting varying degrees of dynamic behavior.

In our evaluation, CATCloud surpasses state-of-the-art autoscaling techniques such as Holt-Winters exponential smoothing (HW), Long Short-Term Memory (LSTM), Kubernetes Vertical Pod Autoscaler, and Autothrottle \pratik{CITE}. CATCloud demonstrates notable improvements in CPU efficiency, achieving up to XX-YY enhancement, and delivering performance gains of up to XX-YY compared to the evaluated baselines.