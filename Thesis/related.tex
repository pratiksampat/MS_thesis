We now review related work on resource scaling for workloads in cloud setting, with a specific focus on CPU allocation management, and the techniques employed to model application behavior to predict future requirements.

Scaling of cloud applications can either be horizontal or vertical. Horizontal scaling is employed when an application's resource requirements exceed that of the system can provide. Vertical scaling on the other hand, allocates for resources within the same system. Unlike horizontal CPU scaling, Vertical CPU autoscalers adjust for resource limits on finer granularity e.g., millicores or fractional vCPUs.

The first type of autoscalers that came exist were based on rules and thresholds. The Kubernetes Vertical Pod Autoscaler (VPA) observes utilization over a period of time and heuristically determines resource limits based on thresholds \cite{noauthor_kubernetes_nodate}. RUBAS \cite{rattihalli_exploring_2019} uses the the sum of the median and the deviation of CPU utilization observations to recommend limits. These approaches while simple to implement often, suffer from issues of accuracy when compared to its counterparts.

Coming to machine learning and reinforcement learning based solution, Autopilot \cite{rzadca_autopilot_2020} employs moving window predictors and recommends based on Machine learning (ML). CPU usage prediction techniques have also been designed \cite{wang_predicting_2021} using Holt-Winters exponential smoothing (HW) and Long Short-Term Memory (LSTM) methods. Sinan \cite{zhang_sinan_2021} uses ML to model for SLO violations, while FIRM \cite{qiu_firm_nodate} uses reinforcement learning to not only react to SLO violations but also identify key microservices. Autothrottle \cite{wang_autothrottle_2023} uses the throttle heuristics to scale for CPUs, and uses reinforcement learning to ascertain targets based on SLOs. Barring a few, ML and RL based solutions boast improvements in accuracy, however require a long time to re-train and have a high inference overhead which leads to lower reactivity.

Shenango \cite{ousterhout_shenango_2019} and Caladan \cite{fried_caladan_2020} unlike from other autoscaling solution is a CPU scheduler that uses techniques of identifying queuing delays to achieve better performance and efficiency. However these solutions require a fundamental redesign and do not allocate for cores in the precision of granularity required by cloud providers for bandwidth sharing.

Lastly, performance aware techniques like Cilantro \cite{bhardwaj_cilantro_2023} aim to use real world metrics such as latency and throughput to build online performance models. Cilantro however, is not directly comparable as it is built to optimize utilization for a fixed cluster setting and not the elastic cloud.

All the related works discussed above make use of surrogate metrics (CPU Utilization, throttle, etc.) to model for performance behavior. CATCloud, does not rely on these proxies and rather implements light weight extensions to the OS to extract runtime information from the scheduler. This approach significantly reduces the need for models required to analyze and predict trends making CATCloud light-weight and highly reactive to sudden load changes.
