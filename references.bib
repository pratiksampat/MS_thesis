
@inproceedings{ousterhout_shenango_2019,
	title = {Shenango: {Achieving} {High} \{{CPU}\} {Efficiency} for {Latency}-sensitive {Datacenter} {Workloads}},
	isbn = {978-1-931971-49-2},
	shorttitle = {Shenango},
	url = {https://www.usenix.org/conference/nsdi19/presentation/ousterhout},
	language = {en},
	urldate = {2024-03-02},
	author = {Ousterhout, Amy and Fried, Joshua and Behrens, Jonathan and Belay, Adam and Balakrishnan, Hari},
	year = {2019},
	pages = {361--378},
}

@inproceedings{wang_predicting_2021,
	address = {New York, NY, USA},
	series = {{EuroMLSys} '21},
	title = {Predicting {CPU} usage for proactive autoscaling},
	isbn = {978-1-4503-8298-4},
	url = {https://doi.org/10.1145/3437984.3458831},
	doi = {10.1145/3437984.3458831},
	abstract = {Private and public clouds require users to specify requests for resources such as CPU and memory (RAM) to be provisioned for their applications. The values of these requests do not necessarily relate to the application's run-time requirements, but only help the cloud infrastructure resource manager to map requested resources to physical resources. If an application exceeds these values, it might be throttled or even terminated. As a consequence, requested values are often overestimated, resulting in poor resource utilization in the cloud infrastructure. Autoscaling is a technique used to overcome these problems. We observed that Kubernetes Vertical Pod Autoscaler (VPA) might be using an autoscaling strategy that performs poorly on workloads that periodically change. Our experimental results show that compared to VPA, predictive methods based on Holt-Winters exponential smoothing (HW) and Long Short-Term Memory (LSTM) can decrease CPU slack by over 40\% while avoiding CPU insufficiency for various CPU workloads. Furthermore, LSTM has been shown to generate stabler predictions compared to that of HW, which allowed for more robust scaling decisions.},
	urldate = {2024-02-29},
	booktitle = {Proceedings of the 1st {Workshop} on {Machine} {Learning} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Thomas and Ferlin, Simone and Chiesa, Marco},
	month = apr,
	year = {2021},
	pages = {31--38},
}

@inproceedings{gan_open-source_2019,
	address = {New York, NY, USA},
	series = {{ASPLOS} '19},
	title = {An {Open}-{Source} {Benchmark} {Suite} for {Microservices} and {Their} {Hardware}-{Software} {Implications} for {Cloud} \& {Edge} {Systems}},
	isbn = {978-1-4503-6240-5},
	url = {https://dl.acm.org/doi/10.1145/3297858.3304013},
	doi = {10.1145/3297858.3304013},
	abstract = {Cloud services have recently started undergoing a major shift from monolithic applications, to graphs of hundreds or thousands of loosely-coupled microservices. Microservices fundamentally change a lot of assumptions current cloud systems are designed with, and present both opportunities and challenges when optimizing for quality of service (QoS) and cloud utilization. In this paper we explore the implications microservices have across the cloud system stack. We first present DeathStarBench, a novel, open-source benchmark suite built with microservices that is representative of large end-to-end services, modular and extensible. DeathStarBench includes a social network, a media service, an e-commerce site, a banking system, and IoT applications for coordination control of UAV swarms. We then use DeathStarBench to study the architectural characteristics of microservices, their implications in networking and operating systems, their challenges with respect to cluster management, and their trade-offs in terms of application design and programming frameworks. Finally, we explore the tail at scale effects of microservices in real deployments with hundreds of users, and highlight the increased pressure they put on performance predictability.},
	urldate = {2024-02-29},
	booktitle = {Proceedings of the {Twenty}-{Fourth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
	month = apr,
	year = {2019},
	keywords = {acceleration, cloud computing, cluster management, datacenters, fpga, microservices, qos, serverless},
	pages = {3--18},
}

@inproceedings{ferdman_clearing_2012,
	address = {New York, NY, USA},
	series = {{ASPLOS} {XVII}},
	title = {Clearing the clouds: a study of emerging scale-out workloads on modern hardware},
	isbn = {978-1-4503-0759-8},
	shorttitle = {Clearing the clouds},
	url = {https://doi.org/10.1145/2150976.2150982},
	doi = {10.1145/2150976.2150982},
	abstract = {Emerging scale-out workloads require extensive amounts of computational resources. However, data centers using modern server hardware face physical constraints in space and power, limiting further expansion and calling for improvements in the computational density per server and in the per-operation energy. Continuing to improve the computational resources of the cloud while staying within physical constraints mandates optimizing server efficiency to ensure that server hardware closely matches the needs of scale-out workloads. In this work, we introduce CloudSuite, a benchmark suite of emerging scale-out workloads. We use performance counters on modern servers to study scale-out workloads, finding that today's predominant processor micro-architecture is inefficient for running these workloads. We find that inefficiency comes from the mismatch between the workload needs and modern processors, particularly in the organization of instruction and data memory systems and the processor core micro-architecture. Moreover, while today's predominant micro-architecture is inefficient when executing scale-out workloads, we find that continuing the current trends will further exacerbate the inefficiency in the future. In this work, we identify the key micro-architectural needs of scale-out workloads, calling for a change in the trajectory of server processors that would lead to improved computational density and power efficiency in data centers.},
	urldate = {2024-02-29},
	booktitle = {Proceedings of the seventeenth international conference on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ferdman, Michael and Adileh, Almutaz and Kocberber, Onur and Volos, Stavros and Alisafaee, Mohammad and Jevdjic, Djordje and Kaynak, Cansu and Popescu, Adrian Daniel and Ailamaki, Anastasia and Falsafi, Babak},
	month = mar,
	year = {2012},
	keywords = {architectural evaluation, cloud computing, design insights, workload characterization},
	pages = {37--48},
}

@misc{noauthor_resource_nodate,
	title = {Resource {Management} for {Pods} and {Containers}},
	url = {https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/},
	abstract = {When you specify a Pod, you can optionally specify how much of each resource a container needs. The most common resources to specify are CPU and memory (RAM); there are others.
When you specify the resource request for containers in a Pod, the kube-scheduler uses this information to decide which node to place the Pod on. When you specify a resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.},
	language = {en},
	urldate = {2024-02-26},
	note = {Section: docs},
}

@misc{noauthor_control_nodate,
	title = {Control {Group} v2 — {The} {Linux} {Kernel} documentation},
	url = {https://docs.kernel.org/admin-guide/cgroup-v2.html#cpu},
	urldate = {2024-02-26},
}

@misc{noauthor_production-grade_nodate,
	title = {Production-{Grade} {Container} {Orchestration}},
	url = {https://kubernetes.io/},
	abstract = {Production-Grade Container Orchestration},
	language = {en},
	urldate = {2024-02-26},
}

@inproceedings{turner_cpu_2010,
	title = {{CPU} bandwidth control for {CFS}},
	url = {https://www.semanticscholar.org/paper/CPU-bandwidth-control-for-CFS-Turner-Rao/c33fca3c4e4f541a5066cc4b9415a75511c05b00},
	abstract = {Over the past few years there has been an increasing focus on the development of features for resource management within the Linux kernel. The addition of the fair group scheduler has enabled the provisioning of proportional CPU time through the specification of group weights. Since the scheduler is inherently workconserving in nature, a task or a group can consume excess CPU share in an otherwise idle system. There are many scenarios where this extra CPU share can cause unacceptable utilization or latency. CPU bandwidth provisioning or limiting approaches this problem by providing an explicit upper bound on usage in addition to the lower bound already provided by shares. There are many enterprise scenarios where this functionality is useful. In particular are the cases of payper-use environments, and latency provisioning within non-homogeneous environments. This paper details the requirements behind this feature, the challenges involved in incorporating into CFS (Completely Fair Scheduler), and the future development road map for this feature. 1 CPU as a manageable resource Before considering the aspect of bandwidth provisioning let us first review some of the basic existing concepts currently arbitrating entity management within the scheduler. There are two major scheduling classes within the Linux CPU scheduler, SCHED\_RT and SCHED\_NORMAL. When runnable, entities from the former, the real-time scheduling class, will always be elected to run over those from the normal scheduling class. Prior to v2.6.24, the scheduler had no notion of any entity larger than that of single task1. The available management APIs reflected this and the primary control of bandwidth available was nice(2). In v2.6.24, the completely fair scheduler (CFS) was merged, replacing the existing SCHED\_NORMAL scheduling class. This new design delivered weight based scheduling of CPU bandwidth, enabling arbitrary partitioning. This allowed support for group scheduling to be added, managed using cgroups through the CPU controller sub-system. This support allows for the flexible creation of scheduling groups, allowing the fraction of CPU resources received by a group of tasks to be arbitrated as a whole. The addition of this support has been a major step in scheduler development, enabling Linux to align more closely with enterprise requirements for managing this resouce. The hierarchies supported by this model are flexible, and groups may be nested within groups. Each group entity’s bandwidth is provisioned using a corresponding shares attribute which defines its weight. Similarly, the nice(2) API was subsumed to control the weight of an individual task entity. Figure 1 shows the hierarchical groups that might be created in a typical university server to differentiate CPU bandwidth between users such as professors, students, and different departments. One way to think about shares is that it provides lowerbound provisioning. When CPU bandwidth is scheduled at capacity, all runnable entities will receive bandwidth in accordance with the ratio of their share weight. It’s key to observe here that not all entities may be runnable 1Recall that under Linux any kernel-backed thread is considered individual task entity, there is no typical notion of a process in scheduling context.},
	urldate = {2024-02-26},
	author = {Turner, Paul and Rao, B. B. and Rao, N.},
	year = {2010},
}

@misc{noauthor_fix_nodate,
	title = {fix \#51135 make {CFS} quota period configurable by szuecs · {Pull} {Request} \#63437 · kubernetes/kubernetes},
	url = {https://github.com/kubernetes/kubernetes/pull/63437},
	abstract = {What this PR does / why we need it:
This PR makes it possible for users to change CFS quota period from the default 100ms to some other value between 1µs and 1s.
\#51135 shows that multiple producti...},
	language = {en},
	urldate = {2024-02-23},
	journal = {GitHub},
}

@misc{noauthor_for_nodate,
	title = {For the {Love} of {God}, {Stop} {Using} {CPU} {Limits} on {Kubernetes} ({Updated}) {\textbar} {Robusta}},
	url = {https://home.robusta.dev/blog/stop-using-cpu-limits},
	abstract = {Many people think you need CPU limits on Kubernetes but this isn't true. In most cases, Kubernetes CPU limits do more harm than help. In fact, they're the number one cause of Kubernetes CPU throttling.},
	language = {en},
	urldate = {2024-02-23},
}

@misc{noauthor_requests_2023,
	title = {Requests are all you need - {CPU} {Limits} and {Throttling} in {Kubernetes}},
	url = {https://www.numeratorengineering.com/requests-are-all-you-need-cpu-limits-and-throttling-in-kubernetes/},
	abstract = {Background

At Numerator, the Deep Learning Team hosts numerous models as a collections microservices on Kubernetes. We have a variety of building blocks in our architecture that allow us to build more complicated pipelines and processing steps. At the core, we have a fairly large number of pods that are},
	language = {en},
	urldate = {2024-02-23},
	journal = {Engineering at Numerator},
	month = jul,
	year = {2023},
}

@inproceedings{bauer_chamulteon_2019,
	title = {Chamulteon: {Coordinated} {Auto}-{Scaling} of {Micro}-{Services}},
	shorttitle = {Chamulteon},
	url = {https://ieeexplore.ieee.org/document/8885153},
	doi = {10.1109/ICDCS.2019.00199},
	abstract = {Nowadays, in order to keep track of the fast changing requirements of Internet applications, auto-scaling is used as an essential mechanism for adapting the number of provisioned resources to the resource demand. The straightforward approach is to deploy a set of common and opensource single-service auto-scalers for each service independently. However, this deployment leads to problems such as bottleneck-shifting and increased oscillations. Existing auto-scalers that scale applications consisting of multiple services are kept closed-source. To face these challenges, we first survey existing auto-scalers and highlight current challenges. Then, we introduce Chamulteon, a redesign of our previously introduced mechanism, which can scale applications consisting of multiple services in a coordinated manner. We evaluate Chamulteon against four different well-cited auto-scalers in four sets of measurement-based experiments where we use diverse environments (VM vs. Docker), real-world traces, and vary the scale of the demanded resources. Overall, Chamulteon achieves the best auto-scaling performance based on established user-oriented and endorsed elasticity metrics.},
	urldate = {2024-02-23},
	booktitle = {2019 {IEEE} 39th {International} {Conference} on {Distributed} {Computing} {Systems} ({ICDCS})},
	author = {Bauer, André and Lesch, Veronika and Versluis, Laurens and Ilyushkin, Alexey and Herbst, Nikolas and Kounev, Samuel},
	month = jul,
	year = {2019},
	note = {ISSN: 2575-8411},
	keywords = {Auto-Scaling, Benchmark testing, Benchmarking, Cloud Computing, Cloud computing, Container, Elasticity, Measurement, Metrics, Predictive models, Queueing analysis, Service Demand Estimation, Time factors, Virtual machining, Workload Forecasting},
	pages = {2015--2025},
}

@article{bauer_chameleon_2019,
	title = {Chameleon: {A} {Hybrid}, {Proactive} {Auto}-{Scaling} {Mechanism} on a {Level}-{Playing} {Field}},
	volume = {30},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {Chameleon},
	url = {https://ieeexplore.ieee.org/document/8465991/},
	doi = {10.1109/TPDS.2018.2870389},
	abstract = {Auto-scalers for clouds promise stable service quality at low costs when facing changing workload intensity. The major public cloud providers provide trigger-based auto-scalers based on thresholds. However, trigger-based auto-scaling has reaction times in the order of minutes. Novel auto-scalers from literature try to overcome the limitations of reactive mechanisms by employing proactive prediction methods. However, the adoption of proactive auto-scalers in production is still very low due to the high risk of relying on a single proactive method. This paper tackles the challenge of reducing this risk by proposing a new hybrid auto-scaling mechanism, called Chameleon, combining multiple different proactive methods coupled with a reactive fallback mechanism. Chameleon employs on-demand, automated time series-based forecasting methods to predict the arriving load intensity in combination with run-time service demand estimation to calculate the required resource consumption per work unit without the need for application instrumentation. We benchmark Chameleon against ﬁve different state-of-the-art proactive and reactive auto-scalers one in three different private and public cloud environments. We generate ﬁve different representative workloads each taken from different real-world system traces. Overall, Chameleon achieves the best scaling behavior based on user and elasticity performance metrics, analyzing the results from 400 hours aggregated experiment time.},
	language = {en},
	number = {4},
	urldate = {2024-02-23},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Bauer, Andre and Herbst, Nikolas and Spinner, Simon and Ali-Eldin, Ahmed and Kounev, Samuel},
	month = apr,
	year = {2019},
	pages = {800--813},
}

@misc{noauthor_ibm_nodate,
	title = {{IBM} {Turbonomic}},
	url = {https://www.ibm.com/products/turbonomic},
	abstract = {IBM Turbonomic software uses AI to optimize the performance, cost and compliance of hybrid cloud and multicloud environments.},
	language = {en-us},
	urldate = {2024-02-22},
}

@article{reiss_heterogeneity_nodate,
	title = {Heterogeneity and {Dynamicity} of {Clouds} at {Scale}: {Google} {Trace} {Analysis}},
	abstract = {To better understand the challenges in developing effective cloudbased resource schedulers, we analyze the ﬁrst publicly available trace data from a sizable multi-purpose cluster. The most notable workload characteristic is heterogeneity: in resource types (e.g., cores:RAM per machine) and their usage (e.g., duration and resources needed). Such heterogeneity reduces the effectiveness of traditional slot- and core-based scheduling. Furthermore, some tasks are constrained as to the kind of machine types they can use, increasing the complexity of resource assignment and complicating task migration. The workload is also highly dynamic, varying over time and most workload features, and is driven by many short jobs that demand quick scheduling decisions. While few simplifying assumptions apply, we ﬁnd that many longer-running jobs have relatively stable resource utilizations, which can help adaptive resource schedulers.},
	language = {en},
	author = {Reiss, Charles and Tumanov, Alexey},
}

@inproceedings{lu_imbalance_2017,
	title = {Imbalance in the cloud: {An} analysis on {Alibaba} cluster trace},
	shorttitle = {Imbalance in the cloud},
	url = {https://ieeexplore.ieee.org/document/8258257},
	doi = {10.1109/BigData.2017.8258257},
	abstract = {To improve resource efficiency and design intelligent scheduler for clouds, it is necessary to understand the workload characteristics and machine utilization in large-scale cloud data centers. In this paper, we perform a deep analysis on a newly released trace dataset by Alibaba in September 2017, consists of detail statistics of 11089 online service jobs and 12951 batch jobs co-locating on 1300 machines over 12 hours. To the best of our knowledge, this is one of the first work to analyze the Alibaba public trace. Our analysis reveals several important insights about different types of imbalance in the Alibaba cloud. Such imbalances exacerbate the complexity and challenge of cloud resource management, which might incur severe wastes of resources and low cluster utilization. 1) Spatial Imbalance: heterogeneous resource utilization across machines and workloads. 2) Temporal Imbalance: greatly time-varying resource usages per workload and machine. 3) Imbalanced proportion of multi-dimensional resources (CPU and memory) utilization per workload. 4) Imbalanced resource demands and runtime statistics (duration and task number) between online service and offline batch jobs. We argue accommodating such imbalances during resource allocation is critical to improve cluster efficiency, and will motivate the emergence of new resource managers and schedulers.},
	urldate = {2024-02-22},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Lu, Chengzhi and Ye, Kejiang and Xu, Guoyao and Xu, Cheng-Zhong and Bai, Tongxin},
	month = dec,
	year = {2017},
	keywords = {Alibaba Trace, Cloud Computing, Cloud computing, Container, Containers, Imbalance, Quality of service, Resource Efficiency, Resource management, Runtime},
	pages = {2884--2892},
}

@misc{noauthor_runtime_nodate,
	title = {runtime: long {GC} {STW} pauses (≥80ms) · {Issue} \#19378 · golang/go},
	shorttitle = {runtime},
	url = {https://github.com/golang/go/issues/19378},
	abstract = {What version of Go are you using (go version)? go1.8 linux/amd64 What operating system and processor architecture are you using (go env)? GOBIN="" GOEXE="" GOHOSTARCH="amd64" GOHOSTOS="linux" GOOS=...},
	language = {en},
	urldate = {2024-02-19},
	journal = {GitHub},
}

@misc{noauthor_avoid_nodate,
	title = {Avoid setting {CPU} limits for {Guaranteed} pods · {Issue} \#51135 · kubernetes/kubernetes},
	url = {https://github.com/kubernetes/kubernetes/issues/51135},
	abstract = {The effect of CPU throttling is non obvious to users and throws them off when they try out kubernetes. It also complicates CPU capacity planning for pods. Pods that are carefully placed in Guarante...},
	language = {en},
	urldate = {2024-02-19},
	journal = {GitHub},
}

@misc{wang_autothrottle_2023,
	title = {Autothrottle: {A} {Practical} {Bi}-{Level} {Approach} to {Resource} {Management} for {SLO}-{Targeted} {Microservices}},
	shorttitle = {Autothrottle},
	url = {http://arxiv.org/abs/2212.12180},
	abstract = {Achieving resource efficiency while preserving end-user experience is non-trivial for cloud application operators. As cloud applications progressively adopt microservices, resource managers are faced with two distinct levels of system behavior: end-to-end application latency and per-service resource usage. Translating between the two levels, however, is challenging because user requests traverse heterogeneous services that collectively (but unevenly) contribute to the endto-end latency. We present Autothrottle, a bi-level resource management framework for microservices with latency SLOs (service-level objectives). It architecturally decouples application SLO feedback from service resource control, and bridges them through the notion of performance targets. Specifically, an application-wide learning-based controller is employed to periodically set performance targets—expressed as CPU throttle ratios—for per-service heuristic controllers to attain. We evaluate Autothrottle on three microservice applications, with workload traces from production scenarios. Results show superior CPU savings, up to 26.21\% over the best-performing baseline and up to 93.84\% over all baselines.},
	language = {en},
	urldate = {2024-02-19},
	publisher = {arXiv},
	author = {Wang, Zibo and Li, Pinghe and Liang, Chieh-Jan Mike and Wu, Feng and Yan, Francis Y.},
	month = oct,
	year = {2023},
	note = {arXiv:2212.12180 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{noauthor_autothrottle_nodate,
	title = {Autothrottle: {A} {Practical} {Bi}-{Level} {Approach} to {Resource} {Management} for {SLO}-{Targeted} {Microservices} {\textbar} {USENIX}},
	url = {https://www.usenix.org/conference/nsdi24/presentation/wang-zibo},
	urldate = {2024-02-19},
}

@inproceedings{zhang_sinan_2021,
	address = {Virtual USA},
	title = {Sinan: {ML}-based and {QoS}-aware resource management for cloud microservices},
	isbn = {978-1-4503-8317-2},
	shorttitle = {Sinan},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446693},
	doi = {10.1145/3445814.3446693},
	abstract = {Cloud applications are increasingly shifting from large monolithic services, to large numbers of loosely-coupled, specialized microservices. Despite their advantages in terms of facilitating development, deployment, modularity, and isolation, microservices complicate resource management, as dependencies between them introduce backpressure effects and cascading QoS violations.},
	language = {en},
	urldate = {2024-02-15},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Zhang, Yanqi and Hua, Weizhe and Zhou, Zhuangzhuang and Suh, G. Edward and Delimitrou, Christina},
	month = apr,
	year = {2021},
	pages = {167--181},
}

@misc{bhat_pratiksampatsleeping-ebizzy_2014,
	title = {pratiksampat/sleeping-ebizzy},
	copyright = {GPL-2.0},
	url = {https://github.com/pratiksampat/sleeping-ebizzy},
	abstract = {Ebizzy benchmark modified to sleep periodically to induce idleness},
	author = {Bhat, Srivatsa},
	month = jun,
	year = {2014},
}

@misc{henson_ebizzy_2008,
	title = {ebizzy {Benchmark}},
	url = {https://openbenchmarking.org/test/pts/ebizzy},
	urldate = {2023-05-09},
	author = {Henson, Valerie},
	month = jan,
	year = {2008},
	note = {https://openbenchmarking.org/test/pts/ebizzy},
}

@inproceedings{bhardwaj_cilantro_2023,
	title = {Cilantro: \{{Performance}-{Aware}\} {Resource} {Allocation} for {General} {Objectives} via {Online} {Feedback}},
	isbn = {978-1-939133-34-2},
	shorttitle = {Cilantro},
	url = {https://www.usenix.org/conference/osdi23/presentation/bhardwaj},
	language = {en},
	urldate = {2024-01-30},
	author = {Bhardwaj, Romil and Kandasamy, Kirthevasan and Biswal, Asim and Guo, Wenshuo and Hindman, Benjamin and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
	year = {2023},
	pages = {623--643},
}

@inproceedings{fried_caladan_2020,
	title = {Caladan: {Mitigating} {Interference} at {Microsecond} {Timescales}},
	isbn = {978-1-939133-19-9},
	shorttitle = {Caladan},
	url = {https://www.usenix.org/conference/osdi20/presentation/fried},
	language = {en},
	urldate = {2024-01-30},
	author = {Fried, Joshua and Ruan, Zhenyuan and Ousterhout, Amy and Belay, Adam},
	year = {2020},
	pages = {281--297},
}

@inproceedings{ali-eldin_adaptive_2012,
	title = {An adaptive hybrid elasticity controller for cloud infrastructures},
	url = {https://ieeexplore.ieee.org/document/6211900},
	doi = {10.1109/NOMS.2012.6211900},
	abstract = {Cloud elasticity is the ability of the cloud infrastructure to rapidly change the amount of resources allocated to a service in order to meet the actual varying demands on the service while enforcing SLAs. In this paper, we focus on horizontal elasticity, the ability of the infrastructure to add or remove virtual machines allocated to a service deployed in the cloud. We model a cloud service using queuing theory. Using that model we build two adaptive proactive controllers that estimate the future load on a service. We explore the different possible scenarios for deploying a proactive elasticity controller coupled with a reactive elasticity controller in the cloud. Using simulation with workload traces from the FIFA world-cup web servers, we show that a hybrid controller that incorporates a reactive controller for scale up coupled with our proactive controllers for scale down decisions reduces SLA violations by a factor of 2 to 10 compared to a regression based controller or a completely reactive controller.},
	urldate = {2024-01-30},
	booktitle = {2012 {IEEE} {Network} {Operations} and {Management} {Symposium}},
	author = {Ali-Eldin, Ahmed and Tordsson, Johan and Elmroth, Erik},
	month = apr,
	year = {2012},
	note = {ISSN: 2374-9709},
	keywords = {Adaptation models, Control systems, Elasticity, Engines, Estimation, Load modeling, Servers},
	pages = {204--212},
}

@article{qu_auto-scaling_2018,
	title = {Auto-{Scaling} {Web} {Applications} in {Clouds}: {A} {Taxonomy} and {Survey}},
	volume = {51},
	issn = {0360-0300},
	shorttitle = {Auto-{Scaling} {Web} {Applications} in {Clouds}},
	url = {https://doi.org/10.1145/3148149},
	doi = {10.1145/3148149},
	abstract = {Web application providers have been migrating their applications to cloud data centers, attracted by the emerging cloud computing paradigm. One of the appealing features of the cloud is elasticity. It allows cloud users to acquire or release computing resources on demand, which enables web application providers to automatically scale the resources provisioned to their applications without human intervention under a dynamic workload to minimize resource cost while satisfying Quality of Service (QoS) requirements. In this article, we comprehensively analyze the challenges that remain in auto-scaling web applications in clouds and review the developments in this field. We present a taxonomy of auto-scalers according to the identified challenges and key properties. We analyze the surveyed works and map them to the taxonomy to identify the weaknesses in this field. Moreover, based on the analysis, we propose new future directions that can be explored in this area.},
	number = {4},
	urldate = {2023-05-13},
	journal = {ACM Computing Surveys},
	author = {Qu, Chenhao and Calheiros, Rodrigo N. and Buyya, Rajkumar},
	month = jul,
	year = {2018},
	keywords = {Auto-scaling, cloud computing, web application},
	pages = {73:1--73:33},
}

@article{lorido-botran_review_2014,
	title = {A {Review} of {Auto}-scaling {Techniques} for {Elastic} {Applications} in {Cloud} {Environments}},
	volume = {12},
	issn = {1570-7873, 1572-9184},
	url = {http://link.springer.com/10.1007/s10723-014-9314-7},
	doi = {10.1007/s10723-014-9314-7},
	language = {en},
	number = {4},
	urldate = {2024-01-30},
	journal = {Journal of Grid Computing},
	author = {Lorido-Botran, Tania and Miguel-Alonso, Jose and Lozano, Jose A.},
	month = dec,
	year = {2014},
	pages = {559--592},
}

@inproceedings{zhao_tiny_2022,
	title = {Tiny {Autoscalers} for {Tiny} {Workloads}: {Dynamic} {CPU} {Allocation} for {Serverless} {Functions}},
	shorttitle = {Tiny {Autoscalers} for {Tiny} {Workloads}},
	url = {http://arxiv.org/abs/2203.00592},
	doi = {10.1109/CCGrid54584.2022.00026},
	abstract = {In serverless computing, applications are executed under lightweight virtualization and isolation environments, such as containers or micro virtual machines. Typically, their memory allocation is set by the user before deployment. All other resources, such as CPU, are allocated by the provider statically and proportionally to memory allocations. This contributes to either under-utilization or throttling. The former signiﬁcantly impacts the provider, while the latter impacts the client. To solve this problem and accommodate both clients and providers, a solution is dynamic CPU allocation achieved through autoscaling. Autoscaling has been investigated for long-running applications using history-based techniques and prediction. However, serverless applications are short-running workloads, where such techniques are not well suited.},
	language = {en},
	urldate = {2023-11-17},
	booktitle = {2022 22nd {IEEE} {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGrid})},
	author = {Zhao, Yuxuan and Uta, Alexandru},
	month = may,
	year = {2022},
	note = {arXiv:2203.00592 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {170--179},
}

@inproceedings{shahrad_serverless_2020,
	title = {Serverless in the {Wild}: {Characterizing} and {Optimizing} the {Serverless} {Workload} at a {Large} {Cloud} {Provider}},
	isbn = {978-1-939133-14-4},
	shorttitle = {Serverless in the {Wild}},
	url = {https://www.usenix.org/conference/atc20/presentation/shahrad},
	language = {en},
	urldate = {2023-05-03},
	author = {Shahrad, Mohammad and Fonseca, Rodrigo and Goiri, Íñigo and Chaudhry, Gohar and Batum, Paul and Cooke, Jason and Laureano, Eduardo and Tresness, Colby and Russinovich, Mark and Bianchini, Ricardo},
	year = {2020},
	pages = {205--218},
}

@misc{noauthor_serverless_nodate,
	title = {Serverless {Compute} {Engine}–{AWS} {Fargate} {Pricing}–{Amazon} {Web} {Services}},
	url = {https://aws.amazon.com/fargate/pricing/},
	abstract = {Learn more about pricing for AWS Fargate. With AWS Fargate you pay only for the compute, memory, and storage resources you use.},
	language = {en-US},
	urldate = {2023-10-24},
	journal = {Amazon Web Services, Inc.},
}

@inproceedings{copik_sebs_2021,
	address = {Québec city Canada},
	title = {{SeBS}: a serverless benchmark suite for function-as-a-service computing},
	isbn = {978-1-4503-8534-3},
	shorttitle = {{SeBS}},
	url = {https://dl.acm.org/doi/10.1145/3464298.3476133},
	doi = {10.1145/3464298.3476133},
	language = {en},
	urldate = {2023-11-17},
	booktitle = {Proceedings of the 22nd {International} {Middleware} {Conference}},
	publisher = {ACM},
	author = {Copik, Marcin and Kwasniewski, Grzegorz and Besta, Maciej and Podstawski, Michal and Hoefler, Torsten},
	month = nov,
	year = {2021},
	pages = {64--78},
}

@misc{noauthor_production-grade_nodate-1,
	title = {Production-{Grade} {Container} {Orchestration}},
	url = {https://kubernetes.io/},
	abstract = {Production-Grade Container Orchestration},
	language = {en},
	urldate = {2023-05-13},
	journal = {Kubernetes},
	note = {https://kubernetes.io/},
}

@article{vu_predictive_2022,
	title = {Predictive {Hybrid} {Autoscaling} for {Containerized} {Applications}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3214985},
	abstract = {One of the main challenges in deploying container service is providing the scalability to satisfy the service performance and avoid resource wastage. To deal with this challenge, Kubernetes provides two kinds of scaling mode: vertical and horizontal. Several existing autoscaling methods make efforts to improve the default autoscalers in Kubernetes; however, most of these works only focus on one scaling mode at the same time, which results in some limitations. Only horizontal scaling may lead to low utilization of containers due to the fixed amount of resources for each instance, especially in the low-request period. In contrast, only vertical scaling may not ensure the quality of service (QoS) requirements in case of bursty workload due to reaching the upper limit. Besides, it is also necessary to provide burst identification for auto-scalers to guarantee service performance. This paper proposes a hybrid autoscaling method with burst awareness for containerized applications. This new approach considers a combination of both vertical and horizontal abilities to satisfy the QoS requirement while optimizing the utilization of containers. Our proposal uses a predictive method based on the machine learning technique to predict the future demand of the application and combines it with a burst identification module, which makes scaling decisions more effective. Experimental results show an enhancement in maintaining the response time below the QoS constraint whereas remaining high utilization of the deployment compared with existing baseline methods in single scaling mode.},
	journal = {IEEE Access},
	author = {Vu, Dinh-Dai and Tran, Minh-Ngoc and Kim, Younghan},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Cloud computing, Forecasting, Kubernetes, Machine learning, Measurement, Predictive models, Quality of service, Resource management, Scalability, autoscaling, machine learning, workload forecasting},
	pages = {109768--109778},
}

@misc{jacobs_optimizing_2023,
	title = {Optimizing kubernetes resource requests/limits},
	url = {https://github.com/zalando/public-presentations},
	abstract = {List of public talks by Zalando Tech: meetup presentations, recorded conference talks, slides},
	urldate = {2023-05-08},
	publisher = {Zalando SE},
	author = {Jacobs, Henngings},
	month = may,
	year = {2023},
	note = {original-date: 2019-05-10T09:52:49Z},
}

@misc{noauthor_kubernetes_nodate,
	title = {kubernetes vertical-pod-autoscaler},
	url = {https://github.com/kubernetes/autoscaler},
	abstract = {Autoscaling components for Kubernetes. Contribute to kubernetes/autoscaler development by creating an account on GitHub.},
	language = {en},
	urldate = {2023-05-03},
	journal = {GitHub},
	note = {https://github.com/kubernetes/autoscaler},
}

@inproceedings{wang_predicting_2021-1,
	address = {Online United Kingdom},
	title = {Predicting {CPU} usage for proactive autoscaling},
	isbn = {978-1-4503-8298-4},
	url = {https://dl.acm.org/doi/10.1145/3437984.3458831},
	doi = {10.1145/3437984.3458831},
	abstract = {Private and public clouds require users to specify requests for resources such as CPU and memory (RAM) to be provisioned for their applications. The values of these requests do not necessarily relate to the application’s run-time requirements, but only help the cloud infrastructure resource manager to map requested resources to physical resources. If an application exceeds these values, it might be throttled or even terminated. As a consequence, requested values are often overestimated, resulting in poor resource utilization in the cloud infrastructure. Autoscaling is a technique used to overcome these problems.},
	language = {en},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 1st {Workshop} on {Machine} {Learning} and {Systems}},
	publisher = {ACM},
	author = {Wang, Thomas and Ferlin, Simone and Chiesa, Marco},
	month = apr,
	year = {2021},
	pages = {31--38},
}

@inproceedings{baresi_kosmos_2021,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{KOSMOS}: {Vertical} and {Horizontal} {Resource} {Autoscaling} for {Kubernetes}},
	isbn = {978-3-030-91431-8},
	shorttitle = {{KOSMOS}},
	doi = {10.1007/978-3-030-91431-8_59},
	abstract = {Cloud applications are increasingly executed onto lightweight containers that can be efficiently managed to cope with highly varying and unpredictable workloads. Kubernetes, the most popular container orchestrator, provides means to automatically scale containerized applications to keep their response time under control. Kubernetes provisions resources using two main components: i) Horizontal Pod Autoscaler (HPA), which controls the amount of containers running for an application, and ii) Vertical Pod Autoscaler (VPA), which oversees the resource allocation of existing containers. These two components have several limitations: they must control different metrics, they use simple threshold-based rules, and the reconfiguration of existing containers requires stopping and restarting them.},
	language = {en},
	booktitle = {Service-{Oriented} {Computing}},
	publisher = {Springer International Publishing},
	author = {Baresi, Luciano and Hu, Davide Yi Xian and Quattrocchi, Giovanni and Terracciano, Luca},
	editor = {Hacid, Hakim and Kao, Odej and Mecella, Massimo and Moha, Naouel and Paik, Hye-young},
	year = {2021},
	keywords = {Containers, Control theory, Kubernetes, Resource provisioning},
	pages = {821--829},
}

@article{qiu_firm_nodate,
	title = {{FIRM}: {An} {Intelligent} {Fine}-{Grained} {Resource} {Management} {Framework} for {SLO}-{Oriented} {Microservices}},
	abstract = {User-facing latency-sensitive web services include numerous distributed, intercommunicating microservices that promise to simplify software development and operation. However, multiplexing of compute resources across microservices is still challenging in production because contention for shared resources can cause latency spikes that violate the servicelevel objectives (SLOs) of user requests. This paper presents FIRM, an intelligent ﬁne-grained resource management framework for predictable sharing of resources across microservices to drive up overall utilization. FIRM leverages online telemetry data and machine-learning methods to adaptively (a) detect/localize microservices that cause SLO violations, (b) identify low-level resources in contention, and (c) take actions to mitigate SLO violations via dynamic reprovisioning. Experiments across four microservice benchmarks demonstrate that FIRM reduces SLO violations by up to 16× while reducing the overall requested CPU limit by up to 62\%. Moreover, FIRM improves performance predictability by reducing tail latencies by up to 11×.},
	language = {en},
	author = {Qiu, Haoran and Banerjee, Subho S and Jha, Saurabh and Kalbarczyk, Zbigniew T and Iyer, Ravishankar K},
}

@inproceedings{rattihalli_exploring_2019,
	title = {Exploring {Potential} for {Non}-{Disruptive} {Vertical} {Auto} {Scaling} and {Resource} {Estimation} in {Kubernetes}},
	doi = {10.1109/CLOUD.2019.00018},
	abstract = {Cloud platforms typically require users to provide resource requirements for applications so that resource managers can schedule containers with adequate allocations. However, the requirements for container resources often depend on numerous factors such as application input parameters, optimization flags, input files, and attributes that are specified for each run. So, it is complex for users to estimate the resource requirements for a given container accurately, leading to resource over-estimation that negatively affects overall utilization. We have designed a Resource Utilization Based Autoscaling System (RUBAS) that can dynamically adjust the allocation of containers running in a Kubernetes cluster. RUBAS improves upon the Kubernetes Vertical Pod Autoscaler (VPA) system non-disruptively by incorporating container migration. Our experiments use multiple scientific benchmarks. We analyze the allocation pattern of RUBAS with Kubernetes VPA. We compare the performance of container migration for in-place and remote node migration and we evaluate the overhead in RUBAS. Our results show that compared to Kubernetes VPA, RUBAS improves the CPU and memory utilization of the cluster by 10\% and reduces the runtime by 15\% with an overhead for each application ranging from 5\% to 20\%.},
	booktitle = {2019 {IEEE} 12th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	author = {Rattihalli, Gourav and Govindaraju, Madhusudhan and Lu, Hui and Tiwari, Devesh},
	month = jul,
	year = {2019},
	note = {ISSN: 2159-6190},
	keywords = {Container migration, Docker, Docker Swarm, Kubernetes, VPA, Vertical Scaling},
	pages = {33--40},
}

@misc{heo_control_2015,
	title = {Control {Group} v2 — {The} {Linux} {Kernel} documentation},
	url = {https://docs.kernel.org/admin-guide/cgroup-v2.html},
	urldate = {2023-05-08},
	author = {Heo, Tejun},
	month = oct,
	year = {2015},
}

@misc{sachidananda_collective_2022,
	title = {Collective {Autoscaling} for {Cloud} {Microservices}},
	url = {http://arxiv.org/abs/2112.14845},
	abstract = {As cloud applications shift from monoliths to loosely coupled microservices, application developers must decide how many compute resources (e.g., number of replicated containers) to assign to each microservice within an application. This decision affects both (1) the dollar cost to the application developer and (2) the end-to-end latency perceived by the application user. Today, individual microservices are autoscaled independently by adding VMs whenever per-microservice CPU or memory utilization crosses a configurable threshold. However, an application user’s end-to-end latency consists of time spent on multiple microservices and each microservice might need a different number of VMs to achieve an overall end-to-end latency. We present COLA, an autoscaler for microservice-based applications, which collectively allocates VMs to microservices with a global goal of minimizing dollar cost while keeping end-to-end application latency under a given target. Using 5 open-source applications, we compared COLA to several utilization and machine learning based autoscalers. We evaluate COLA across different compute settings on Google Kubernetes Engine (GKE) in which users manage compute resources, GKE standard, and a new mode of operation in which the cloud provider manages compute infrastructure, GKE Autopilot. COLA meets a desired median or tail latency target on 53 of 63 workloads where it provides a cost reduction of 19.3\%, on average, over the next cheapest autoscaler. COLA is the most cost effective autoscaling policy for 48 of these 53 workloads. The cost savings from managing a cluster with COLA result in COLA paying for its training cost in a few days. On smaller applications, for which we can exhaustively search microservice configurations, we find that COLA is optimal for 90\% of cases and near optimal otherwise.},
	language = {en},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Sachidananda, Vighnesh and Sivaraman, Anirudh},
	month = aug,
	year = {2022},
	note = {arXiv:2112.14845 [cs, eess]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{thurgood_cloud_2019,
	address = {New York, NY, USA},
	series = {{ICFNDS} '19},
	title = {Cloud {Computing} {With} {Kubernetes} {Cluster} {Elastic} {Scaling}},
	isbn = {978-1-4503-7163-6},
	url = {https://dl.acm.org/doi/10.1145/3341325.3341995},
	doi = {10.1145/3341325.3341995},
	abstract = {Cloud computing and artificial intelligence (AI) technologies are becoming increasingly prevalent in the industry, necessitating the requirement for advanced platforms to support their workloads through parallel and distributed architectures. Kubernetes provides an ideal platform for hosting various workloads, including dynamic workloads based on AI applications that support ubiquitous computing devices leveraging parallel and distributed architectures. The rationale is that Kubernetes can be used to support backend services running on parallel and distributed architectures, hosting ubiquitous cloud computing workloads. These applications support smart homes and concerts, providing an environment that automatically scales based on demand. While Kubernetes does offer support for auto scaling of Pods to support these workloads, automated scaling of the cluster itself is not currently offered. In this paper we introduce a Free and Open Source Software (FOSS) solution for autoscaling Kubernetes (K8s) worker nodes within a cluster to support dynamic workloads. We go on to discuss scalability issues and security concerns both on the platform and within the hosted AI applications.},
	urldate = {2023-05-01},
	booktitle = {Proceedings of the 3rd {International} {Conference} on {Future} {Networks} and {Distributed} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Thurgood, Brandon and Lennon, Ruth G.},
	month = jul,
	year = {2019},
	keywords = {Artificial Intelligence, Autoscaling, Container as a Service, Infrastructure as a Service, Kubernetes, parallel and distributed architectures},
	pages = {1--7},
}

@misc{edb-msft_autoscale_2023,
	title = {Autoscale in {Azure} {Monitor} - {Azure} {Monitor}},
	url = {https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-overview},
	abstract = {This article describes the autoscale feature in Azure Monitor and its benefits.},
	language = {en-us},
	urldate = {2023-05-11},
	author = {EdB-MSFT},
	month = mar,
	year = {2023},
	note = {https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-overview},
}

@inproceedings{balla_adaptive_2020,
	title = {Adaptive scaling of {Kubernetes} pods},
	doi = {10.1109/NOMS47738.2020.9110428},
	abstract = {Scalability is the ability of a resource or an application to be expanded to handle the increasing demands. One of the most emphasized aspects of cloud computing is the ability of scaling up or down. Scaling can be performed by either an operator or automatically. Autoscaling algorithms are getting more and more emphasized in the field of cloud computing. In this paper we propose an adaptive autoscaler, Libra, which automatically detects the optimal resource set for a single pod, then manages the horizontal scaling process. Additionally, if the load or the underlying virtualized environment changes, Libra adapts the resource definition for the pod and adjusts the horizontal scaling process accordingly.},
	booktitle = {{NOMS} 2020 - 2020 {IEEE}/{IFIP} {Network} {Operations} and {Management} {Symposium}},
	author = {Balla, David and Simon, Csaba and Maliosz, Markosz},
	month = apr,
	year = {2020},
	note = {ISSN: 2374-9709},
	keywords = {Cloud Computing, Cloud computing, Couplings, Instruction sets, Resource Management, Scalability, Web services},
	pages = {1--5},
}

@misc{noauthor_aws_nodate,
	title = {{AWS} {Auto} {Scaling}},
	url = {https://docs.aws.amazon.com/autoscaling/plans/userguide/what-is-a-scaling-plan.html},
	urldate = {2023-05-11},
	note = {https://docs.aws.amazon.com/autoscaling/plans/userguide/what-is-a-scaling-plan.html},
}

@inproceedings{rzadca_autopilot_2020,
	address = {New York, NY, USA},
	series = {{EuroSys} '20},
	title = {Autopilot: workload autoscaling at {Google}},
	isbn = {978-1-4503-6882-7},
	shorttitle = {Autopilot},
	url = {https://dl.acm.org/doi/10.1145/3342195.3387524},
	doi = {10.1145/3342195.3387524},
	abstract = {In many public and private Cloud systems, users need to specify a limit for the amount of resources (CPU cores and RAM) to provision for their workloads. A job that exceeds its limits might be throttled or killed, resulting in delaying or dropping end-user requests, so human operators naturally err on the side of caution and request a larger limit than the job needs. At scale, this results in massive aggregate resource wastage. To address this, Google uses Autopilot to configure resources automatically, adjusting both the number of concurrent tasks in a job (horizontal scaling) and the CPU/memory limits for individual tasks (vertical scaling). Autopilot walks the same fine line as human operators: its primary goal is to reduce slack - the difference between the limit and the actual resource usage - while minimizing the risk that a task is killed with an out-of-memory (OOM) error or its performance degraded because of CPU throttling. Autopilot uses machine learning algorithms applied to historical data about prior executions of a job, plus a set of finely-tuned heuristics, to walk this line. In practice, Autopiloted jobs have a slack of just 23\%, compared with 46\% for manually-managed jobs. Additionally, Autopilot reduces the number of jobs severely impacted by OOMs by a factor of 10. Despite its advantages, ensuring that Autopilot was widely adopted took significant effort, including making potential recommendations easily visible to customers who had yet to opt in, automatically migrating certain categories of jobs, and adding support for custom recommenders. At the time of writing, Autopiloted jobs account for over 48\% of Google's fleet-wide resource usage.},
	urldate = {2023-05-01},
	booktitle = {Proceedings of the {Fifteenth} {European} {Conference} on {Computer} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Rzadca, Krzysztof and Findeisen, Pawel and Swiderski, Jacek and Zych, Przemyslaw and Broniek, Przemyslaw and Kusmierek, Jarek and Nowak, Pawel and Strack, Beata and Witusowski, Piotr and Hand, Steven and Wilkes, John},
	month = apr,
	year = {2020},
	pages = {1--16},
}
